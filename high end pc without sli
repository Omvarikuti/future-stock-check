import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow import keras
from keras import layers
from sklearn.preprocessing import MinMaxScaler
import keras_tuner as kt
from keras.utils import plot_model
import joblib
import os
import multiprocessing
import time

# Setup
os.makedirs("stress_output", exist_ok=True)
seq_length = 100
total_points = 500_000  # insanely large fake dataset
future_steps = 100

# Generate synthetic data to max out memory
print("Generating synthetic dataset...")
y = np.sin(np.linspace(0, 1000, total_points)).reshape(-1, 1)
scaler = MinMaxScaler()
y_scaled = scaler.fit_transform(y)

def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

print("Creating sequences...")
X_seq, y_seq = create_sequences(y_scaled, seq_length)
X_seq = X_seq[:200000]  # cap to avoid memory crash
y_seq = y_seq[:200000]
print(f"Sequence shape: {X_seq.shape}")

# Model definition
def build_model():
    model = keras.Sequential()
    model.add(layers.Input(shape=(seq_length, 1)))
    for _ in range(4):  # stack LSTM layers
        model.add(layers.LSTM(256, return_sequences=True))
    model.add(layers.LSTM(256))
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# Training function for parallel runs
def train_model(model_id):
    model = build_model()
    print(f"[Model {model_id}] Training...")
    history = model.fit(X_seq, y_seq, epochs=10, batch_size=512, verbose=1)
    model.save(f"stress_output/model_{model_id}.h5")
    pd.DataFrame(history.history).to_csv(f"stress_output/history_{model_id}.csv", index=False)
    print(f"[Model {model_id}] Done.")

# Parallel training
num_processes = min(multiprocessing.cpu_count(), 4)
processes = []
for i in range(num_processes):
    p = multiprocessing.Process(target=train_model, args=(i,))
    p.start()
    processes.append(p)

for p in processes:
    p.join()

print("ðŸ”¥ ALL MODELS FINISHED TRAINING ðŸ”¥")

# Constant plotting to add heat
print("Generating massive plots...")
for i in range(20):
    plt.figure(figsize=(20, 10))
    for j in range(50):
        plt.plot(np.random.randn(1000).cumsum())
    plt.savefig(f"stress_output/plot_{i}.png")
    plt.close()

print("ðŸ’¾ Finalizing output...")

# Final disk I/O dump
joblib.dump(scaler, 'stress_output/scaler.pkl')
np.save('stress_output/X_seq.npy', X_seq)
np.save('stress_output/y_seq.npy', y_seq)
print("âœ… Done. MacBook should be sweating.")
